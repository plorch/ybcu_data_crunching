---
title: "DataImport"
author: "Patrick D. lorch"
date: "2022-12-06"
output: html_document
---

## Analysis history

I started with trying to combine all 14 years into one dataset.  Each year or range (2014-2018) is imported, some columns are renamed, some variables are transformed.  In some cases, I had to go to other tables and join in the data to get something comparable to what was in 2014-2018 files.  Once I did this for all years, I began focusing on 2019-2022 to use in an occupancy model to help with Buereau of Reclamation summary report for those years.

Initially, all the import and reformat code was in one place and the code to reformat and summarize by site (with detection counts) was done in another section.  As the code blocks have gotten longer, I moved the summary code up to be with the import and reformat code, since it is easier to re-run it this way.

## Libraries

```{r libraries}
library(DBI)
# library(RPostgreSQL)
library(RPostgres)
library(dplyr)
library(tidyr)
library(rgdal)
library(sp)
# library(sf)
library(unmarked)
library(ggplot2)
```

## Background

* C1507 - (Beal) SSRS only surveyed in 2019, Reclamation covered remaining years. 
* C1801 - (Planet Ranch) SSRS surveyed it 2020 and 2022, Reclamation covered it in 2019. 
* C1906 - (Cougar Point) only one survey conducted in 2019 which showed it was no longer suitable YBCU habitat and was not surveyed again. I haven't been including this site in the summary report as a result. 
* C1936 - Gibraltar Rock, it was not surveyed 2020-2022 due to lack of suitable habitat
* C1938 - (Sandy Wash) only one survey conducted in 2021 before fire destroyed the habitat, no surveys conducted there for remainder of contract. 
* C2748 - (Eastside) Site was surveyed in 2021 but the section code did not exist. It was surveyed as part of C2741 survey and the data was later extracted for reporting purposes. I could manually relabel the survey/visit/point keys for the 2021 Eastside data if you'd like, it's only a like 2-5 survey points per survey. 
* C4963 - (LDCA Reach 2) Surveyed by two surveyors in 2019, split into Reach 2 North (C4967) and Reach 2 South (C4968) for 2020-2022. 

## Data import

I imported any table I could from access .mdb and .accdb files directly as a table for each year into the postgis database (postgis_32_ssrs:ybcu_lcr).  This will allow me to use dplyr or sql commands to combine data into a view that combines years even though table headers are different.  

In dBeaver, use these steps to move a table to the database:

1. Connect the access dB to the dBeaver using 

  a. Add connection button (brings up add wizard)
  b. Choose either MSAccess (UCanAccess) or CSV and hit Next
  c. Open button will allow you to locate the Access file or csv directory
  d. Hit Finish
  
2. Expand new connection in Database Navigator and expand down to list of tables
3. Right click the table you wan to import and choose Export data
4. For export target choose Database (default) and hit Next
5. Make sure the target database and schema are selected at the top
6. Change the target table name, if needed
7. Accept defaults for everything else and choose Proceed

This should result in this table showing up in the postgres database.

I could not do the same with the data in shapefiles for 2020-2021.  These had been saved as .csv files, but with some unknown coordinate system (x = -12768058.497299999, y = 3948568.8812000006, for example. Shows as WGS 1984 pseudo mercator in .prj file). It looks like I could just divide these by 100000, but I just used st transform instead.  Shapefiles were imported into R first using rgdal, then exported as .csv and added to the postgis database using the method above.

Once in postgis, I can import the data to data.frames from there.

### Shapefile processing

Uses rgdal::readOGR or read.csv to bring in data from shapefiles.
*** This will need to be transitioned to using sf rather than sp package ***

2021 data uses updated survey point and detection tables where Cari Lyn split out data for a site that was added last minute (eventually called C2741) and entered initially into another site (C2748).  just changed the Survey Key, Visit Key, and Point Keys of the Eastside data to include the C2748 section code. She did not change the territory keys since she didn't think I was using them for this analysis, nor did she change the point numbers. This is why a second set of survey point and detection tables were imported.

Shapefile import is truncating the Date Collected field (which has a comma in it).  

*** Don't use the spdf versions for the time calcuations etc. ***

I am falling back to importing from csv directly into R.  For now these are not converted to sf or sp objects, since we don't need that for occupancy.

```{r shapefiles}

lcr_ybcu_survey_points_df_2019 <- read.csv("C:/Users/PatrickLorch/SSRS/Southern Sierra Research Station - Documents/Projects/LCR YBCU/Data/2019/Data/2019 .csv files/Survey Point.csv")
# This is just to get the proj4string 
#   "+proj=utm +zone=11 +datum=NAD83 +units=m +no_defs"
dsn19 = "C:/Users/PatrickLorch/SSRS/Southern Sierra Research Station - Documents/Projects/LCR YBCU/Data/2019/Data/2019 Shapefiles"
lcr_ybcu_survey_points_spdf_2019 = readOGR(dsn = dsn19, layer = "surveypoint2019")
crs2019 = proj4string(lcr_ybcu_survey_points_spdf_2019)
# We do not need to transform this one since it is in UTMs

lcr_ybcu_survey_points_df_2020 <- read.csv("C:/Users/PatrickLorch/SSRS/Southern Sierra Research Station - Documents/Projects/LCR YBCU/Data/2020/Data/2020 .csv files/Survey Point (1).csv")
# This is to check the crs fortransformation.
dsn20 = "C:/Users/PatrickLorch/SSRS/Southern Sierra Research Station - Documents/Projects/LCR YBCU/Data/2020/Data/2020 Shapefiles"
lcr_ybcu_survey_points_spdf_2020 = readOGR(dsn = dsn20, layer = "SurPoint")
crs2020 = proj4string(lcr_ybcu_survey_points_spdf_2020)
# Make spdf and set coordinates from @data
coordinates(lcr_ybcu_survey_points_df_2020) = ~x + y
# Set crs
proj4string(lcr_ybcu_survey_points_df_2020) = proj4string(lcr_ybcu_survey_points_spdf_2020)
# Transform to UTM
lcr_ybcu_survey_points_spdf_UTM11_2020 = 
  spTransform(lcr_ybcu_survey_points_df_2020, 
              CRS(crs2019))
# This puts UTM coordinates back into the data file
lcr_ybcu_survey_points_spdf_UTM11_2020$x11E =
  sp::coordinates(lcr_ybcu_survey_points_spdf_UTM11_2020)[,1]
lcr_ybcu_survey_points_spdf_UTM11_2020$y11N =
  sp::coordinates(lcr_ybcu_survey_points_spdf_UTM11_2020)[,2]

# Using updated file from Cari Lynn
# dsn21 = "C:/Users/PatrickLorch/SSRS/Southern Sierra Research Station - Documents/Projects/LCR YBCU/Data/2021/Data/2021 Shapefiles"
# lcr_ybcu_survey_points_spdf_2021 = readOGR(dsn = dsn21, layer = "Survey Point")
# proj4string(lcr_ybcu_survey_points_spdf_2021)
# sp::coordinates(lcr_ybcu_survey_points_spdf_2021)

lcr_ybcu_survey_points_df_2021 <- read.csv("C:/Users/PatrickLorch/SSRS/Southern Sierra Research Station - Documents/Projects/LCR YBCU/Data/2021/Data/Data edited to include C2748/Survey Point 2021 corrected times.csv")

# This is to check the crs for transformation.
dsn21a = "C:/Users/PatrickLorch/SSRS/Southern Sierra Research Station - Documents/Projects/LCR YBCU/Data/2021/Data/Data edited to include C2748"
lcr_ybcu_survey_points_spdf_2021a = readOGR(dsn = dsn21a, layer = "SurveyPoint2021")
crs2021 = proj4string(lcr_ybcu_survey_points_spdf_2021a)
# Make spdf and set coordinates from @data
coordinates(lcr_ybcu_survey_points_df_2021) = ~x + y
# Set crs
proj4string(lcr_ybcu_survey_points_df_2021) = proj4string(lcr_ybcu_survey_points_spdf_2021a)
# Transform to UTM
lcr_ybcu_survey_points_spdf_UTM11_2021 = 
  spTransform(lcr_ybcu_survey_points_df_2021, 
              CRS(crs2019))
# This puts UTM coordinates back into the data file
lcr_ybcu_survey_points_spdf_UTM11_2021$x11E =
  sp::coordinates(lcr_ybcu_survey_points_spdf_UTM11_2021)[,1]
lcr_ybcu_survey_points_spdf_UTM11_2021$y11N =
  sp::coordinates(lcr_ybcu_survey_points_spdf_UTM11_2021)[,2]

lcr_ybcu_detections_df_2021a <- read.csv("C:/Users/PatrickLorch/SSRS/Southern Sierra Research Station - Documents/Projects/LCR YBCU/Data/2021/Data/Data edited to include C2748/Detection 2021 corrected times.csv")
# This is to check the crs for transformation.
lcr_ybcu_detections_spdf_2021a = readOGR(dsn = dsn21a, layer = "Detection2021")
crs2021 = proj4string(lcr_ybcu_detections_spdf_2021a)
# Make spdf and set coordinates from @data
coordinates(lcr_ybcu_detections_df_2021a) = ~x + y
# Set crs
proj4string(lcr_ybcu_detections_df_2021a) = proj4string(lcr_ybcu_detections_spdf_2021a)
# Transform to UTM
lcr_ybcu_detections_spdf_UTM11_2021 = 
  spTransform(lcr_ybcu_detections_df_2021a, 
              CRS(crs2019))
# This puts UTM coordinates back into the data file
lcr_ybcu_detections_spdf_UTM11_2021$x11E =
  sp::coordinates(lcr_ybcu_detections_spdf_UTM11_2021)[,1]
lcr_ybcu_detections_spdf_UTM11_2021$y11N =
  sp::coordinates(lcr_ybcu_detections_spdf_UTM11_2021)[,2]

lcr_ybcu_survey_points_df_2022 <- read.csv("C:/Users/PatrickLorch/SSRS/Southern Sierra Research Station - Documents/Projects/LCR YBCU/Data/2022/.csv files/Survey Point.csv")
# Make spdf and set coordinates from @data
coordinates(lcr_ybcu_survey_points_df_2022) = ~x + y
# Set crs
proj4string(lcr_ybcu_survey_points_df_2022) = crs2021
lcr_ybcu_survey_points_spdf_UTM11_2022 = 
  spTransform(lcr_ybcu_survey_points_df_2022, 
              CRS(crs2019))
# This puts UTM coordinates back into the data file
lcr_ybcu_survey_points_spdf_UTM11_2022$x11E =
  sp::coordinates(lcr_ybcu_survey_points_spdf_UTM11_2022)[,1]
lcr_ybcu_survey_points_spdf_UTM11_2022$y11N =
  sp::coordinates(lcr_ybcu_survey_points_spdf_UTM11_2022)[,2]

# Not used since we now import from .csv files
# lcr_ybcu_survey_points_spdf_UTM11_2020 = 
#   spTransform(lcr_ybcu_survey_points_spdf_2020, 
#               CRS(proj4string(lcr_ybcu_survey_points_spdf_2019)))
# lcr_ybcu_survey_points_spdf_UTM11_2020$x =
#   coordinates(lcr_ybcu_survey_points_spdf_UTM11_2020)[,1]
# lcr_ybcu_survey_points_spdf_UTM11_2020$y =
#   coordinates(lcr_ybcu_survey_points_spdf_UTM11_2020)[,2]

# lcr_ybcu_survey_points_spdf_UTM11_2021 = 
#   spTransform(lcr_ybcu_survey_points_spdf_2021, 
#               CRS(proj4string(lcr_ybcu_survey_points_spdf_2019)))
# lcr_ybcu_survey_points_spdf_UTM11_2021$x =
#   coordinates(lcr_ybcu_survey_points_spdf_UTM11_2021)[,1]
# lcr_ybcu_survey_points_spdf_UTM11_2021$y =
#   coordinates(lcr_ybcu_survey_points_spdf_UTM11_2021)[,2]

# lcr_ybcu_survey_points_spdf_UTM11_2021a = 
#   spTransform(lcr_ybcu_survey_points_spdf_2021a, 
#               CRS(proj4string(lcr_ybcu_survey_points_spdf_2019)))
# lcr_ybcu_survey_points_spdf_UTM11_2021a$x =
#   sp::coordinates(lcr_ybcu_survey_points_spdf_UTM11_2021a)[,1]
# lcr_ybcu_survey_points_spdf_UTM11_2021a$y =
#   sp::coordinates(lcr_ybcu_survey_points_spdf_UTM11_2021a)[,2]

# lcr_ybcu_detections_spdf_UTM11_2021a = 
#   spTransform(lcr_ybcu_detections_spdf_2021a, 
#               CRS(proj4string(lcr_ybcu_survey_points_spdf_2019)))
# lcr_ybcu_detections_spdf_UTM11_2021a$x =
#   sp::coordinates(lcr_ybcu_detections_spdf_UTM11_2021a)[,1]
# lcr_ybcu_detections_spdf_UTM11_2021a$y =
#   sp::coordinates(lcr_ybcu_detections_spdf_UTM11_2021a)[,2]

writeOGR(lcr_ybcu_survey_points_spdf_UTM11_2020,
         dsn = "stuff",
         layer = "survey_points_2020_UTM11N",
         driver = "CSV")
# writeOGR(lcr_ybcu_survey_points_spdf_UTM11_2021,
#          dsn = "stuff2",
#          layer = "survey_points_2021_UTM11N",
#          driver = "CSV")
writeOGR(lcr_ybcu_survey_points_spdf_UTM11_2021,
         dsn = "stuff2a",
         layer = "survey_points_2021_UTM11N",
         driver = "CSV")
writeOGR(lcr_ybcu_detections_spdf_UTM11_2021,
         dsn = "stuff2b",
         layer = "detections_2021_UTM11N",
         driver = "CSV")
writeOGR(lcr_ybcu_survey_points_spdf_UTM11_2022,
         dsn = "stuff3",
         layer = "survey_points_2022_UTM11N",
         driver = "CSV")

```

### Import survey points for all years and summarize by site for occupancy modeling
#### Data gathering notes

To help interpret datasets from different years, here is what I understand.

* 2008-2014 paper forms and GPS
  * 2012 was a fully normalized database that John set up in MySQL?
* 2014-2018 Trimble Junos (2014 entered into Junos?)
* 2019-2021 Arc Collector on phones and tablets.
* 2022 Arc Field Maps on phones and tablets.

#### Connect to postgis database

First bring data in from tables in postgis database.

```{r connect}
source("connect.R")

tryCatch({
    # drv <- dbDriver("PostgreSQL")
    drv <- RPostgres::Postgres()
    print("Connecting to Database…")
    connec <- dbConnect(drv, 
                 dbname = dsn_database,
                 host = dsn_hostname, 
                 port = dsn_port,
                 user = dsn_uid, 
                 password = dsn_pwd,
                 options="-c search_path=ybcu_lcr"
                 )
    print("Database Connected!")
    },
    error=function(cond) {
            print("Unable to connect to Database.")
    })

# get a bunch of tables
points08 = dbReadTable(connec, "points08")
points09 = dbReadTable(connec, "points09")
points10 = dbReadTable(connec, "points10")
points11 = dbReadTable(connec, "points11")
points12 = dbReadTable(connec, "points12")
points13 = dbReadTable(connec, "points13")
points1418 = dbReadTable(connec, "points1418")
points19 = dbReadTable(connec, "points19")
points20 = dbReadTable(connec, "points20")
points21 = dbReadTable(connec, "points21")
points22 = dbReadTable(connec, "points22")

surveys1418 = dbReadTable(connec, "surveys1418")
surveys12 = dbReadTable(connec, "surveys12")
pointdetection12 = dbReadTable(connec, "pointdetection12")
cicada12 = dbReadTable(connec, "cicada12")
surveypoint12 = dbReadTable(connec, "surveypoint12")
surveydetections1418 = dbReadTable(connec, "surveydetections1418")
surveys11 = dbReadTable(connec, "surveys11")
surveys10 = dbReadTable(connec, "surveys10")
surveys08 = dbReadTable(connec, "surveys08")
surveys20 = dbReadTable(connec, "surveys20")

visits19 = dbReadTable(connec, "visits19")
visits20 = dbReadTable(connec, "visits20")
visits21 = dbReadTable(connec, "visits21")
visits22 = dbReadTable(connec, "visits22")

detections19 = dbReadTable(connec, "detections19")
detections20 = dbReadTable(connec, "detections20")
detections21 = dbReadTable(connec, "detections21")
detections22 = dbReadTable(connec, "detections22")

dbDisconnect(conn = connec)
```

#### Aborted for now:  Append dataframes together

*** This is not as easy as I had thought and I abandoned trying to get all 14 years together in one frame for now. ***

Then append yearly dataframes together adding year. We started with points1418 since it has multiple years and already has the structure we want.

We need to map column names in a transparent way, so we do that first.  Some may need to be converted or coded before combining. For example, cic_ind was an index of Cicada abundance used from 2014-2018. In other years, a text field with a count or a range was given. Also some times are character, most are POSIXct

The 2014-2018 dataset (and some others) does not have survey period assigned.  Different observers used different date ranges for survey numbering, so we cannot assign them based on date range. I tried grouping and sorting by date then numbering based on order, but there are two observers working within a loccode at different points, which makes this method fail. There are also examples (see 2013 below) of one observer doing different points in the same loccode at different times in the day, using overlapping point numbers, so this also creates a problem.

***Note:*** If we need survey period for all years.  We may just have to go through the 2015 (possibly some other years) and assign them manually, and then use the non-overlapping start and end dates in sv_periods1418 for 2016-2018. 2014 already had period recorded. It is not recorded in 2011, afaikt. 

#### Cicada data notes

* in 2014-2018 codes where used for cic_ind from 0-5 representing
  * "0"     "1"     "2-5"   "6-10"  "11-19" ">20"
* in 2019-2022 the actual ranges represented by the index (0-5) was recorded
* The table cicada12 has this in table form and I added the codes for joining


#### Summarize by site

These playback surveys have some features that make modeling effort difficult:

  * If a cuckoo is detected, surveyors move along the transect line 300 m from the estimated location of the detected cuckoo.  
    * This results in skipping potential survey points
    * This makes using survey point total a bad measure of effort
  * If a cuckoo is detected, playback is supposed to stop
    * This makes using playback number a bad measure of effort
    * The ratio of the actual number to the potential number of playbacks might be useful
  * ***Transect length, potential survey point count, or site area might be worth adding to the detection model***
  
Ideally, we get a site level summary of detection for multiple years, then combine them into a matrix of 0 and 1s for each site for each period and year.

Year would then be the primary sampling period and survey period the secondary for dynamic occupancy models.

I have added survey_duration_approx which is the min and max time for points in a site/section.

***Errors***
When you run the summary the first time with date in group_by(), the following errors are found.

#### 2014-2018

Survey detection tables will have multiple rows per point if more than one bird is detected.

My strategy for now is to eliminate any that are marked as Repeat (det_ty = R) and count what is left. That is what the svdetN1418 block does.

```{r combine1418}
# Make more interpretable names with all lower case and no spaces
# Remove data on gps or data entry device and storage

# counts and detection type (Repeat, New) are in sv_dets_1418 as $det_ty
# No clear key exists to match with survey point
#   points1418$mscp_pk is unique
#   Some of these overlap with surveydetections1418$mscp_pk, but it has many more
#     It may only be 2014 that matches up.
#   Some of these seem to be malformed, with extra stuff in them

table(points1418$point_id)
table(points1418$mscp_pk)
length(unique(points1418$mscp_pk)) #9316 == number of rows; All are unique
table(points1418$ybcu_det_q)
length(unique(points1418$mscp_pk[points1418$ybcu_det_q == 1])) # 2031
table(surveydetections1418$mscp_pk, surveydetections1418$det_ty)
length(unique(surveydetections1418$mscp_pk)) # 2753 == number of rows; All are unique

# Code to check block below is doing what we want
svdet1418 = surveydetections1418 %>% 
  dplyr::select("sur_pt", "det_ty", "ybcu_nu", "contractor_folder") %>%
  mutate(detection_key = paste(contractor_folder, sur_pt, sep="_")) %>%
  # arrange(detection_key, ybcu_nu)
  count(detection_key, det_ty, name = "det_count")

# Count the new detections.  We can then filter by this greater than 0 to remove repeat detections
svdetN1418 = surveydetections1418 %>% 
  select("sur_pt", "det_ty", "ybcu_nu", "contractor_folder") %>%
  mutate(detection_key = paste(contractor_folder, sur_pt, sep="_")) %>%
  count(detection_key, wt = (det_ty == "N"), name = "det_count")

names(points1418)
pt1418 = points1418 %>%
  select("svyear", "easting", "loccode", "northing", "sur_pt_date",
         "sur_pt_time", "sur_per", "sur_pt_nu", "cic_ind", "nu_plays",
         "ybcu_det_q", "notes", "point_id", "mscp_pk", "meff_code",
         "contractor_folder") %>%
  mutate(detection_key = paste(contractor_folder,
                               sur_pt_nu, sep="_")) %>%
  left_join(svdetN1418, by = "detection_key") %>%
  rename(svpointdate = sur_pt_date,
         svpointtime = sur_pt_time,
         svperiod = sur_per,
         svpointnumber = sur_pt_nu,
         cicada_index = cic_ind,
         numberofplaybacks = nu_plays,
         ybcudetection = ybcu_det_q,
         ybcudetectionnumber = det_count) # %>%
# Filtering these out removes an absence point without new ybcu from the analysis, 
#   so leaving in for now. Zero for ybcudetectionnumber could mean no ybcu or incidental
  # filter(is.na(ybcudetectionnumber) | ybcudetectionnumber != 0)

names(pt1418)

# Changing this to 0 since observer was unsure
pt1418$ybcudetection[which(points1418$ybcu_det_q == -1)] = 0

# Bringing in Survey periods
#  They overlap, so these cannot be used to assign survey period
sv_periods1418 = surveys1418 %>%
  group_by(sv_year, sur_per) %>%
  summarise(FirstDate = min(start_date), LastDate = max(start_date))

# These were attempts to assign survey period that failed as explained above
# test = pt1418 %>%
#   group_by(svyear, loccode, svpointdate) %>%
#   # arrange(svpointdate) %>%
#   mutate(svperiod2 = 1:length(svpointdate))
```

#### 2013

To distinguish cases where the same surveyor went to different parts of a site at different times and used the same point number (for this year and 2008-2010), we have added ptsvvisitno to detection_key.  Surveyors are supposed to use different visit numbers.


```{r combine13}
names(points13)
# This one splits detection question and count into two columns
# Whether it is a new bird or repeat does not seem to have been recorded this year
pt13 = points13 %>%
  select("ptsitecode", "ptutm11e", "ptdate", "ptsrvr", "ptstarttime",
         "ptutm11n", "ptsvperno", "ptnumber", "indexcicadas", "ptnumplays",
         "detyn", "ptdettype", "ptybcudetnum", "ptnote", "ptsvvisitno") %>%
  rename(loccode = "ptsitecode", 
         easting = "ptutm11e", 
         svpointdate = "ptdate", 
         svpointtime = "ptstarttime", 
         northing = "ptutm11n", 
         svperiod = "ptsvperno", 
         svpointnumber = "ptnumber", 
         cicada_index = "indexcicadas", 
         numberofplaybacks = "ptnumplays", 
         ybcudetection = "detyn",
         ybcudetectioncount = "ptybcudetnum", 
         notes = "ptnote") %>%
  mutate(detection_key = paste(ptsrvr, 
                               loccode, 
                               svpointdate, 
                               ptsvvisitno, 
                               svpointnumber, sep = "_"))

# Problems may still exist with this method when on one day, and site the same point
#   number was used for different locations by the same surveyor.
pt13_det = pt13 %>% select(detection_key, ybcudetection, 
                                 ptdettype, ybcudetectioncount) %>% 
  # filter(ybcudetection == 1)%>%
  arrange(detection_key, ybcudetectioncount)

pt13_det_count = pt13_det %>%
  count(detection_key, wt = (ybcudetection == 1 & ptdettype == "S"), name = "det_count")
table(pt13_det_count$det_count)

pt13 = pt13 %>% left_join(pt13_det_count)

pt13_distinct = pt13 %>% distinct(detection_key, .keep_all = T)

```


#### 2012

For some reason, just this year, the database was mostly normalized. This may be the year John Stanek set up an online MySQL database for data entry.

***pd_ybcu_detection_number is not a count.  It may be a sequential number. I think we still need to count the detections at a point.***

A new point was recorded for each detection at a location.  This means that point_id is unique for each detection, so it cannot be used to count detections at one location.

It looks like something like survey_id, gps_number, and point_gps_waypt may be able to be used as unique identifier.  Is this the best we can do? This will not work perfectly as some surveyors added letters to one number for gps waypoint for the same coordinates.

The pointdetection table is duplicated in the database I was given. I removed the duplicates.

We have counts for now, but they are likely lumping some locations that in other years would not be lumped.

```{r combine12}
names(points12)
# This year they only recorded detection/no detection in points 
# Number needs to be pulled out of the pointdetection table.
# Date needs to be pulled out of the survey12 table.
# Cicada data recorded also must be pulled in.
# There is pointdetection type (Survey and Incidental), and detection type
#   but not whether it is New or a Repeat.

# Remove duplicate rows
pointdetection12 = pointdetection12[!duplicated(pointdetection12),]

pt12 = points12 %>%
  select("point_id", "survey_id", "gps_number", "point_start_time",
         "point_gps_waypt", "point_easting", "point_northing",
         "habitat_type", "structure_type", "point_broadcast_plays", 
         "point_ybcu_detected_yn", "point_note") %>%
  left_join(select(surveys12, "survey_id", "site_code", "visit_type",
                   "survey_date","survey_all_surveyors")) %>%
  left_join(select(pointdetection12, "point_id", "ptdetection_type",
                   "pd_ybcu_detection_number")) %>%
  left_join(select(surveypoint12, "point_id", "cicada_code")) %>%
  rename(easting = "point_easting", 
         svpointtime = "point_start_time",
         northing = "point_northing",
         numberofplaybacks = "point_broadcast_plays",
         ybcudetection = "point_ybcu_detected_yn",
         notes = "point_note",
         loccode = "site_code",
         svpointdate = "survey_date",
         ybcudetectioncount = "pd_ybcu_detection_number",
         cicada_index = "cicada_code")
# Still need to convert svpointtime from character to time to match weird times of other years
pt12$svpointtime = strptime(paste0("1899-12-30 ", pt12$svpointtime), "%Y-%m-%d %H:%M")

# This does not count at points but just survey_id by gps_number combos with S for ptdetection_type
#   This undoubtedly lumps some points together yielding higher counts
pt12_det = pt12 %>% select(survey_id, gps_number, point_id,
                           survey_all_surveyors, ybcudetection,
                           ptdetection_type, ybcudetectioncount) %>% 
  # filter(ybcudetection == 1)%>%
  arrange(survey_id, gps_number, ybcudetectioncount) 

pt12_det_count = pt12_det %>%
  count(survey_id, gps_number, wt = (ptdetection_type == "S"), name = "det_count")
table(pt12_det_count$det_count)

pt12 = pt12 %>% left_join(pt12_det_count)

pt12_distinct = pt12 %>% distinct(survey_id, gps_number, .keep_all = T)
  
```

##### Find close points

This is a boondoggle.  But the plot is a nice tool.

```{r closeones}
library(sf)
library(nngeo)
library(leaflet)

pt12_sf = pt12 %>% st_as_sf(coords = c("easting", "northing"), crs = 6340)
pt12_sf_4326 = pt12_sf %>% st_transform(crs = 4326)

pal <- colorNumeric(
  palette = "magma",
  domain = pt12_sf_4326$ybcudetection,
  na.color = "#737373")

leaflet(pt12_sf_4326) %>%
  addTiles(group = "OSM (default)") %>%
  addProviderTiles('Esri.WorldImagery',group = "Esri World Imagery") %>%
  addCircleMarkers(
    color =  pal(pt12_sf_4326$ybcudetection),
    clusterOptions = markerClusterOptions(),
    popup = ~paste(as.character(survey_id),
                   as.character(gps_number),
                   as.character(point_id),
                   sep = "; "),
    label = ~as.character(survey_id),
    labelOptions = labelOptions(permanent = T)
  ) %>%
  addLayersControl(
        baseGroups = c("OSM (default)", "Esri World Imagery"),
        position = "bottomright",
        options = layersControlOptions(collapsed = TRUE)) %>%
  addLegend(pal = pal, values = pt12_sf_4326$ybcudetection) %>%
  addScaleBar(
    position = c("bottomleft"),
    options = scaleBarOptions()
  )

# Not clear how to use these distances to build groups
pt12_sf[1:40,] %>% group_by(survey_id) %>% st_nn(., ., k=2, returnDist = T)
   
```


#### 2011

I am not seeing that there are any multiple detections per point (as in 2013?). There is either no new survey detection or 1. 

There are no rows with point_ybcu_detected_yn == 1. Using ptdetectiontype to count detections.

```{r combine11}
names(points11)
# 
surveys11 = surveys11 %>% mutate(survey_all_surveyors = paste(survey_surveyor1, 
                                      survey_surveyor2, 
                                      survey_surveyor3, 
                                      survey_surveyor4))

pt11 = points11 %>%
  select("point_id", "survey_date", "survey_id", "point_easting",
         "point_start_time", "point_northing", "point_broadcast_plays",
         "point_ybcu_detected_yn", "ptdetection_type", "site_code",
         "pd_ybcu_detection_number", "cicada_code", "point_note") %>%
  left_join(select(surveys11, "survey_id", survey_all_surveyors)) %>%
  rename(easting = "point_easting", 
         svpointtime = "point_start_time",
         northing = "point_northing",
         numberofplaybacks = "point_broadcast_plays",
         ybcudetection = "point_ybcu_detected_yn",
         notes = "point_note",
         loccode = "site_code",
         svpointdate = "survey_date",
         ybcudetectioncount = "pd_ybcu_detection_number",
         cicada_index = "cicada_code",
         ptdettype = "ptdetection_type") %>%
  mutate(detection_key = paste(survey_all_surveyors,
                               survey_id,
                               svpointdate,
                               point_id, sep = "_"))

pt11_det = pt11 %>% select(detection_key, ybcudetection, 
                                 ptdettype, ybcudetectioncount) %>% 
  # filter(ybcudetection == 1)%>%
  arrange(detection_key, ybcudetectioncount) 

pt11_det_count = pt11_det %>%
  count(detection_key, wt = (ptdettype == "S"), name = "det_count")
table(pt11_det_count$det_count)

pt11 = pt11 %>% left_join(pt11_det_count)

pt11_distinct = pt11 %>% distinct(detection_key, .keep_all = T)

```


#### 2010

***For 2008-2009 we need to count multiple detections still.***

From Shannon McNeil: 
I am responsible for the early years: 2008-2010 we used my Access database, then John took over data management in 2011-2012 with his online MySQL (I think) database, which we then downloaded/imported into Access to match the 2008-2010 data…

Without looking at the database I could be wrong but.. I’m not sure we had a field for “number of cuckoos per point”, but you could probably summarize a table to get the count of detections per point…

ptvistype was used to distinguish surveys (0), followups (1), capture (2), so 0 are new detections.

ptsvvisitno was used to distinguish two surveyors covering parts of a site with overlapping ptnumbers

Added these two variables into analysis and 

```{r combine10}
# Trying to count number of unique detections per point
pt_check = points10 %>%
  filter(ptvistype == 0) %>%
  count(ptsitecode, ptdate, ptnumber, ptsvvisitno)
table(pt_check$n)

pt10 = points10 %>%
  select("ptnumber", "ptdate", "ptsrvr", "ptsvvisitno", "ptutm11e", "ptsvno",
         "ptstarttime", "ptutm11n", "ptsvperno", "ptvistype", "ptnumplays",
         "detyn","ptsitecode", "ptybcudetnum", "indexcicadas", "ptnote") %>%
  rename(easting = "ptutm11e", 
         svpointtime = "ptstarttime",
         northing = "ptutm11n",
         numberofplaybacks = "ptnumplays",
         ybcudetection = "detyn",
         svperiod = "ptsvperno",
         notes = "ptnote",
         loccode = "ptsitecode",
         svpointdate = "ptdate",
         ybcudetectioncount = "ptybcudetnum",
         cicada_index = "indexcicadas") %>%
  mutate(detection_key = paste(ptsrvr,
                               loccode,
                               svpointdate,
                               ptsvvisitno,
                               ptnumber, sep = "_"))

pt10_det = pt10 %>% select(detection_key, ybcudetection, 
                                 ptvistype, ybcudetectioncount) %>% 
  # filter(ybcudetection == 1)%>%
  arrange(detection_key, ybcudetectioncount) 

pt10_det_count = pt10_det %>%
  count(detection_key, wt = (ybcudetection == 1 & ptvistype == 0), name = "det_count")
table(pt10_det_count$det_count)

pt10 = pt10 %>% left_join(pt10_det_count)

pt10_distinct = pt10 %>% distinct(detection_key, .keep_all = T)

```


#### 2009

There are two points tables in the access db for 09.  One has ptincid (svPoints09), the other has ptvistype (lcrybcuPts09).  I treat these the same.

```{r combine09}
pt09 = points09 %>%
  select("ptnumber", "ptdate", "ptsrvr", "ptsvvisitno", "ptutme", "ptsvno",
         "ptstarttime", "ptutmn", "ptsvperno", "ptincid", "ptnumplays",
         "detyn", "ptsitecode", "ptybcudetnum", "indexcicadas", "ptnote") %>%
  rename(easting = "ptutme", 
         svpointtime = "ptstarttime",
         northing = "ptutmn",
         numberofplaybacks = "ptnumplays",
         ybcudetection = "detyn",
         svperiod = "ptsvperno",
         notes = "ptnote",
         loccode = "ptsitecode",
         svpointdate = "ptdate",
         ybcudetectioncount = "ptybcudetnum",
         cicada_index = "indexcicadas") %>%
  mutate(detection_key = paste(ptsrvr,
                               loccode,
                               svpointdate,
                               ptsvvisitno,
                               ptnumber, sep = "_"))

pt09_det = pt09 %>% select(detection_key, ybcudetection, 
                                 ptincid, ybcudetectioncount) %>% 
  # filter(ybcudetection == 1)%>%
  arrange(detection_key, ybcudetectioncount) 

pt09_det_count = pt09_det %>%
  count(detection_key, wt = (ybcudetection == 1 & ptincid == 0), name = "det_count")
table(pt09_det_count$det_count)

pt09 = pt09 %>% left_join(pt09_det_count)

pt09_distinct = pt09 %>% distinct(detection_key, .keep_all = T)

```


#### 2008

Surveyor id (ptsrvr in 09 tables) was put in surveyor08 table.  Primary key for points08 involves gps which was not recorded the same way in the surveys08 table, requiring a bunch of extra work.  However, once we have that, we don't really need to join the surveys table to get surveyor, for example, since the primary key combo is unique.

This year it does not seem like they recorded whether the detection was a survey or incidental detection.  If it is someplace I cannot find it, we will need to add that someplace. Then counts need to be added.


```{r combine08}
pt08 = points08 %>%
  select("ptnumber", "ptdate", "ptgpsno", "ptsvvisitno", "ptutme",  "ptsvno",
         "ptstarttime", "ptutmn", "ptsvperno", "ptnumplays", "detyn",
         "ptsitecode", "ptybcudetnum", "indexcicadas", "ptnote") %>%
  rename(easting = "ptutme", 
         svpointtime = "ptstarttime",
         northing = "ptutmn",
         numberofplaybacks = "ptnumplays",
         ybcudetection = "detyn",
         svperiod = "ptsvperno",
         notes = "ptnote",
         loccode = "ptsitecode",
         svpointdate = "ptdate",
         ybcudetectioncount = "ptybcudetnum",
         cicada_index = "indexcicadas") %>%
  mutate(detection_key = paste(ptgpsno,
                               loccode,
                               svpointdate,
                               ptsvvisitno,
                               ptnumber, sep = "_"))

pt08_det = pt08 %>% select(detection_key, ybcudetection, 
                                 ybcudetectioncount) %>% 
  # filter(ybcudetection == 1)%>%
  arrange(detection_key, ybcudetectioncount) 

pt08_det_count = pt08_det %>%
  count(detection_key, wt = (ybcudetection == 1), name = "det_count")
table(pt08_det_count$det_count)

pt08 = pt08 %>% left_join(pt08_det_count)

pt08_distinct = pt08 %>% distinct(detection_key, .keep_all = T)

```


[Yesterday 4:12 PM] Cari Lynn Squibb
For 2019-2022 data, non-survey data is the follow-up data, which is visit data collected outside of the survey visit. So sometimes all a surveyor does in a day is a follow-up visit at a site to try and resight a bird, other times a surveyor will do a follow-up before their survey (a pre-dawn), then conduct their survey visit, followed by another follow-up visit at the same site to track down the birds they detected on their survey. Non-survey points are sometimes collected during a follow-up visit if the biologist does playback or wants to record a spot where they sat for a looooong time and did not hear a cuckoo. Resight detections are their own feature and can be collected during surveys or follow-ups.

[Yesterday 4:16 PM] Cari Lynn Squibb
To get the number of new, unique survey YBCU detections for the 2019-2022 data you just need to filter the Detection features for Visit Type = Survey and New Bird = Yes. Detections relate to their respective Survey Points via the Point Key.

#### 2019

We have to do it differently, but it is basically the same as 2011.

Survey period and visit number sometimes align, but sometimes do not.  For the years 2019-2022 at least, point key takes into account the visit number which should separate cases where a site is surveyed in two or more parts.

***Errors:***

* 1 test row with no key. This is removed on input.
* FY19-YBC-C2362-SUR-V2-P07 and FY19-YBC-C2368-FOL-V16-P02 Only coordinates and time differ.
* FY19-YBC-C2368-SUR-V4-P02 One entry may have wrong key. Point.Number is 1 not 2
* FY19-YBC-C2546-SUR-V1-P09 One entry has wrong key.  vthorpe change point from 9 to 10.


```{r combine19}
names(points19)

# Fixes for errors found when summarizing
# Test filter criteria:
points19 %>%
  filter(objectid == 1344)

points19_wfixes = points19 %>%
  mutate(Survey.Period = if_else(grepl("C1938", Survey.Key) & 
         grepl("6/27/2019", Date.Collected) &
         Survey.Period == "2",
         "1", Survey.Period)) %>%
  mutate(Survey.Period = if_else(grepl("C2362", Survey.Key) &
         Survey.Period == "",
         "3", Survey.Period)) %>%
  mutate(Survey.Period = if_else(grepl("C4963", Survey.Key) & 
         grepl("7/3/2019", Date.Collected) &
         Survey.Period == "3",
         "2", Survey.Period)) %>%
  mutate(Survey.Period = if_else(grepl("C4965", Survey.Key) & 
         grepl("6/20/2019", Date.Collected) &
         Survey.Period == "2",
         "1", Survey.Period)) %>%
  mutate(Survey.Period = if_else(grepl("C4965", Survey.Key) & 
         grepl("7/4/2019", Date.Collected) &
         Survey.Period == "3",
         "2", Survey.Period)) %>%
  # mutate(Survey.Period = if_else(objectid == 1344,
  #        "2", Survey.Period),
  #        Survey.Key = if_else(objectid == 1344,
  #        "FY19-YBC-C2369", Survey.Key)) %>%
  filter(!objectid %in% c(1950, 1344))


cicada12$cicada_cat = unique(points19$Cicada.Count)

svdet19 = detections19 %>%
  filter(Visit.Type == "Survey", New.Bird == "Yes") %>%
  select(Point.Key, Survey.Period, YBCU.Number) %>%
  arrange(Survey.Period, Point.Key, YBCU.Number) 
pt19_det_count = svdet19 %>%
  count(Point.Key, name = "det_count")

pt19 = points19_wfixes %>%
  select("Survey.Key", "Visit.Key", "Point.Key", "Survey.Period", "Point.Number",
         "Visit.Number", "Cicada.Count", "Play.Count", "YBCU.Detected", 
         "notes", "Date.Collected", "x", "y") %>%
  rename(svperiod = "Survey.Period", 
         numberofplaybacks = "Play.Count", 
         ybcudetection = "YBCU.Detected", 
         svpointtime = "Date.Collected",
         easting = "x", 
         northing = "y") %>%
  separate("Survey.Key",
           c("year", "species","loccode1922"),
           extra = 'drop',
           remove = F) %>%
  select(-year, -species) %>%
  mutate(svpointdate = as.Date(svpointtime, "%m/%d/%Y")) %>%
  left_join(select(cicada12, cicada_index = cicada_code, cicada_cat),
                                  by = c("Cicada.Count" = "cicada_cat")) %>%
  left_join(pt19_det_count, by = "Point.Key") %>%
  # Don't need this now that Date.collected is fixed
  # left_join(select(visits19, Visit.Key, visithours =
  #                    "Total.Number.of.Visit.Hours"), 
  #           by = c("Visit.Key" = "Visit.Key")) %>%
  mutate(det_count = replace_na(det_count, 0))

```

##### Summarize by site, 2019

I have gone back to where I imported points19 to fix these in points19 (code block combine19) so the changes carry through.

* Nothing done:C1906 has only one survey period, so one line in the summary
* C1938 has two entries for survey period 2, but date matches survey period one
  * Done: probably should have survey period changed to 1
* C2362 has an entry with no survey period but date matches survey period 3
  * Done: probably should add survey period 3 to that one
* C2369 has two entries for survey period 2, but date matches survey period 3
  * Done: probably should add survey period 3 to that one
* C4963 has two entries for survey period 3, but date matches survey period 2
  * Done: probably should have survey period changed to 2
* C4965 has one entry marked survey period 2 on 2019-06-20 
  * Done: that should be switched to period 1
  * Done: the one marked with survey period 3 on 2019-07-04 should be switched to 2
* Two rows with no loccode
  * Done: One with date 2019-07-25 could be given Loccode C2362 and survey period 3, based on date and coordinates
  * Removed: The other one does not fit nicely anywhere obvious
  

```{r sitesum19}
# Calculate most common item in categorical variable
Mode = function(x) {
  ux = unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

Scale = function(x) {
  (x - mean(x, na.rm = T)) / sd(x, na.rm = T)
}

# Using svpointdate in the group_by will catch problems in data that need to be sorted out
pt19_sum_site = pt19 %>%
  group_by(loccode1922, svperiod, svpointdate) %>%
  summarise(points_surveyed = n(),
            playbacks_sum = sum(as.numeric(numberofplaybacks)),
            det_count = sum(as.numeric(det_count)),
            survey_date = first(as.Date(svpointdate)),
            survey_duration_approx = 
              as.numeric(max(as.POSIXct(svpointtime, format = "%m/%d/%Y, %I:%M %p")) -
                         min(as.POSIXct(svpointtime, format = "%m/%d/%Y, %I:%M %p")),
                         units = "secs"),
            mean_easting = mean(as.numeric(easting)),
            mean_northing = mean(as.numeric(northing)),
            cicada_class = Mode(cicada_index))

table(pt19_sum_site$loccode1922, pt19_sum_site$svperiod)
# This shows that survey period will probably matter
table(pt19_sum_site$svperiod, pt19_sum_site$det_count)

pt19_sitebysvperiod = pt19_sum_site %>% 
  select(svperiod, det_count, loccode1922) %>%
  pivot_wider(names_from = svperiod, values_from = det_count) %>%
  tibble::column_to_rownames("loccode1922")

pt19_effort_time = pt19_sum_site %>%
  select(svperiod, survey_duration_approx, loccode1922) %>%
  # mutate(effort_time = Scale(survey_duration_approx)) %>%
  pivot_wider(names_from = svperiod, values_from = survey_duration_approx) 

pt19_effort_time_scaled = pt19_effort_time %>%
  select(-1) %>%
  tibble::column_to_rownames("loccode1922") %>%
  scale()

table(pt19$loccode1922, pt19$cicada_index, pt19$svperiod)

pt19_cicada_ind = pt19 %>%
  select(loccode1922, cicada_index, svperiod) %>%
  group_by(loccode1922, svperiod) %>%
  summarise(mean_cic_ind = mean(cicada_index))

pt19_cicada_ind_byperiod = pt19_cicada_ind %>%
  pivot_wider(names_from = svperiod,
              values_from = mean_cic_ind) 

pt19_cicada_ind_byperiod_scaled = pt19_cicada_ind_byperiod %>%
  tibble::column_to_rownames("loccode1922") %>%
  scale()
```



#### 2020

Survey point time was not recorded.  I use visit time from visits20 table.

Problems:

* There is row in the summary that has NA for survey site.  
  * Looks like it should be 2365 survey period 2, based on ins point key and date
* A test row needs to be deleted

```{r combine20}
names(points20)

points20_wfixes = points20 %>% 
  slice(-1) %>%
  mutate(Survey.key = if_else(Survey.key == "", 
                              "FY20-YBC-C2365", Survey.key))
  
svdet20 = detections20 %>%
  filter(Visit.type == "Survey", New.bird. == "Yes") %>%
  select(Point.key, Survey.period, YBCU.number) %>%
  arrange(Survey.period, Point.key, YBCU.number) 
pt20_det_count = svdet20 %>%
  count(Point.key, name = "det_count")

# Removing row which is test row
pt20 = points20_wfixes %>%
  select("Survey.key", "Visit.key", "Point.key", "Survey.period",
         "Visit.number", "Visit.type", "Detection.method",
         "Point.number", "Cicada.count", "Play.count", "YBCU.detected.",
         "notes", "Date.Collected", "x11e", "y11n") %>%
  rename(svperiod = "Survey.period", 
         numberofplaybacks = "Play.count", 
         ybcudetection = "YBCU.detected.", 
         svpointdate = "Date.Collected",
         easting = "x11e", 
         northing = "y11n") %>%
  separate("Survey.key",
           c("year", "species","loccode1922"),
           extra = 'drop',
           remove = F) %>%
  select(-year, -species) %>%
  left_join(select(cicada12, cicada_index = cicada_code, cicada_cat),
                                  by = c("Cicada.count" = "cicada_cat")) %>%
  left_join(pt20_det_count, by = c("Point.key" = "Point.key")) %>%
  # left_join(select(visits20, Visit.key, visithours =
  #                    "Total.number.of.visit.hours"), 
  #           by = c("visit_key" = "Visit.key")) %>%
  mutate(det_count = replace_na(det_count, 0))
pt20[713,]
```

##### Summarize by site, 2020

I have gone back to where I imported points20 to fix these in points20 (code block combine20) so the changes carry through.

Problems:

* There is row in the summary that has NA for survey site.  
  * Done: Looks like it should be 2365 survey period 2 based on point_key and date
* Done: Remove blank test row

```{r sitesum20}
pt20_sum_site = pt20 %>%
  mutate(svdate = 
           as.Date(as.POSIXct(svpointdate, 
                              format = "%m/%d/%Y, %I:%M %p"))) %>%
  group_by(loccode1922, svperiod, svdate) %>%
  summarise(points_surveyed = n(),
            playbacks_sum = sum(as.numeric(numberofplaybacks)),
            det_count = sum(as.numeric(det_count)),
            survey_date = 
              first(svdate),
            survey_duration_approx = 
              as.numeric(max(as.POSIXct(svpointdate, format = "%m/%d/%Y, %I:%M %p")) -
                         min(as.POSIXct(svpointdate, format = "%m/%d/%Y, %I:%M %p")),
                         units = "secs"),
            mean_easting = mean(as.numeric(easting)),
            mean_northing = mean(as.numeric(northing)),
            cicada_class = Mode(cicada_index))

table(pt20_sum_site$loccode1922, pt20_sum_site$svperiod)
table(pt20_sum_site$svperiod, pt20_sum_site$det_count)

pt20_sitebysvperiod = pt20_sum_site %>% 
  select(svperiod, det_count, loccode1922) %>%
  pivot_wider(names_from = svperiod, values_from = det_count) %>%
  tibble::column_to_rownames("loccode1922")

pt20_effort_time = pt20_sum_site %>%
  select(svperiod, survey_duration_approx, loccode1922) %>%
  # mutate(effort_time = Scale(survey_duration_approx)) %>%
  pivot_wider(names_from = svperiod, values_from = survey_duration_approx) 

pt20_effort_time_scaled = pt20_effort_time %>%
  select(-1) %>%
  tibble::column_to_rownames("loccode1922") %>%
  scale()

table(pt20$loccode1922, pt20$cicada_index, pt20$svperiod)

pt20_cicada_ind = pt20 %>%
  select(loccode1922, cicada_index, svperiod) %>%
  group_by(loccode1922, svperiod) %>%
  summarise(mean_cic_ind = mean(cicada_index))

pt20_cicada_ind_byperiod = pt20_cicada_ind %>%
  pivot_wider(names_from = svperiod,
              values_from = mean_cic_ind) 

pt20_cicada_ind_byperiod_scaled = pt20_cicada_ind_byperiod %>%
  tibble::column_to_rownames("loccode1922") %>%
  scale()

```



#### 2021

```{r combine21}
names(points21)
names(detections21)

svdet21 = detections21 %>%
  filter(Visit.type == "Survey", New.bird. == "Yes") %>%
  select(Point.key, Survey.period, YBCU.number) %>%
  arrange(Survey.period, Point.key, YBCU.number) 
pt21_det_count = svdet21 %>%
  count(Point.key, name = "det_count")

# Somehow, Cicada.cat got changed to dates (opened in excel)
points21_fixed = points21%>%
  mutate(Cicada.count = if_else(Cicada.count == "5-Feb",
                                "2-5", 
                                Cicada.count)) %>%
  mutate(Cicada.count = if_else(Cicada.count == "19-Nov",
                                "11-19", 
                                Cicada.count)) %>%
  mutate(Cicada.count = if_else(Cicada.count == "10-June",
                                "6-10", 
                                Cicada.count))


points21%>%
  slice(which(Visit.key == ""))

pt21 = points21_fixed %>%
  slice(which(Survey.period != "0")) %>%
  slice(which(Visit.key != "")) %>%
  select("Survey.key", "Visit.key", "Point.key", "Survey.period",
         "Point.number", "Cicada.count", "Play.count", "YBCU.detected.", 
         "notes", "Date.Collected", "x11e", "y11n") %>%
  rename(svperiod = "Survey.period", 
         numberofplaybacks = "Play.count", 
         ybcudetection = "YBCU.detected.", 
         svpointdate = "Date.Collected",
         easting = "x11e", 
         northing = "y11n") %>%
  separate("Survey.key",
           c("year", "species","loccode1922"),
           extra = 'drop',
           remove = F) %>%
  select(-year, -species) %>%
         
  left_join(select(cicada12, cicada_index = cicada_code, cicada_cat),
                                  by = c("Cicada.count" = "cicada_cat")) %>%
  left_join(pt21_det_count, by = c("Point.key" = "Point.key")) %>%
  # left_join(select(visits21, Visit.key, visithours =
  #                    "Total.number.of.visit.hours"), 
  #           by = c("visit_key" = "Visit.key")) %>%
  mutate(det_count = replace_na(det_count, 0))
pt21[c(197, 299, 303),]

```

##### Summarize by site, 2021

I have gone back to where I imported points21 to fix these in points21 (code block combine21) so the changes carry through.

Problems:

* Three rows with NA for loccode1922
  * Done: Removed all with survey period == 0
  
```{r sitesum21}
pt21_sum_site = pt21 %>%
  mutate(svdate = 
           as.Date(as.POSIXct(svpointdate, 
                              format = "%m/%d/%Y, %I:%M %p"))) %>%
  group_by(loccode1922, svperiod, svdate) %>%
  summarise(points_surveyed = n(),
            playbacks_sum = sum(as.numeric(numberofplaybacks)),
            det_count = sum(as.numeric(det_count)),
            survey_date = 
              first(svdate),
            survey_duration_approx = 
              as.numeric(max(as.POSIXct(svpointdate, format = "%m/%d/%Y, %I:%M %p")) -
                         min(as.POSIXct(svpointdate, format = "%m/%d/%Y, %I:%M %p")),
                         units = "secs"),
            mean_easting = mean(as.numeric(easting)),
            mean_northing = mean(as.numeric(northing)),
            cicada_class = Mode(cicada_index))

table(pt21_sum_site$loccode1922, pt21_sum_site$svperiod)
table(pt21_sum_site$svperiod, pt21_sum_site$det_count)


pt21_sitebysvperiod = pt21_sum_site %>% 
  select(svperiod, det_count, loccode1922) %>%
  pivot_wider(names_from = svperiod, values_from = det_count) %>%
  tibble::column_to_rownames("loccode1922")

pt21_effort_time = pt21_sum_site %>%
  select(svperiod, survey_duration_approx, loccode1922) %>%
  # mutate(effort_time = Scale(survey_duration_approx)) %>%
  pivot_wider(names_from = svperiod, values_from = survey_duration_approx) 

pt21_effort_time_scaled = pt21_effort_time %>%
  select(-1) %>%
  tibble::column_to_rownames("loccode1922") %>%
  scale()

table(pt21$loccode1922, pt21$cicada_index, pt21$svperiod)

pt21_cicada_ind = pt21 %>%
  select(loccode1922, cicada_index, svperiod) %>%
  group_by(loccode1922, svperiod) %>%
  summarise(mean_cic_ind = mean(cicada_index))

pt21_cicada_ind_byperiod = pt21_cicada_ind %>%
  pivot_wider(names_from = svperiod,
              values_from = mean_cic_ind) 

pt21_cicada_ind_byperiod_scaled = pt21_cicada_ind_byperiod %>%
  tibble::column_to_rownames("loccode1922") %>%
  scale()

```


#### 2022


```{r combine22}
names(points22)

points22_wfixes = points22 %>% 
  slice(-c(1232, 1234)) %>%
  mutate(Survey.period = if_else(Visit.key == "FY22-YBC-C1507-Sur-V3",
                              "3", Survey.period)) %>%
  mutate(Survey.period = if_else(Visit.key == "FY22-YBC-C1507-Sur-V4",
                              "4", Survey.period))

svdet22 = detections22 %>%
  filter(Visit.type == "Survey", New.bird. == "Yes") %>%
  select(Point.key, Survey.period, YBCU.number) %>%
  arrange(Survey.period, Point.key, YBCU.number) 
pt22_det_count = svdet22 %>%
  count(Point.key, name = "det_count")

pt22 = points22_wfixes %>%
  select("Survey.key", "Visit.key", "Point.key", "Survey.period", 
         "Point.number", "Cicada.count", "Play.count", 
         "YBCU.detected.", "notes", "Date.Collected", "x11e", 
         "y11n") %>%
  rename(svperiod = "Survey.period", 
         numberofplaybacks = "Play.count", 
         ybcudetection = "YBCU.detected.", 
         svpointtime = "Date.Collected",
         easting = "x11e", 
         northing = "y11n") %>%
  separate("Survey.key",
           c("year", "species","loccode1922"),
           extra = 'drop',
           remove = F) %>%
  select(-year, -species) %>%
  mutate(svpointdate = as.Date(svpointtime, "%m/%d/%Y")) %>%
         
  left_join(select(cicada12, cicada_index = cicada_code, cicada_cat),
                                  by = c("Cicada.count" = "cicada_cat")) %>%
  left_join(pt22_det_count, by = "Point.key") %>%
  left_join(select(visits22, Visit.key, visithours =
                     "Total.number.of.visit.hours"), 
            by = c("Visit.key" = "Visit.key")) %>%
  mutate(det_count = replace_na(det_count, 0))
pt22[c(1232, 1234),]
```

##### Summarize by site, 2022

I have gone back to where I imported points21 to fix these in points21 (code block combine21) so the changes carry through.

Problems:

* two rows with NA for loccode1922
  * Done: Should be deleted
* One record with C1507 on 2022-07-20 is marked as period 2
  * Done: should be changed to period 3
* One record with C1507 on 2022-08-03 is marked as period 3
  * Done: should be changed to period 4

```{r sitesum22}
pt22_sum_site = pt22 %>%
  mutate(svdate = 
           as.Date(as.POSIXct(svpointdate, 
                              format = "%m/%d/%Y, %I:%M %p"))) %>%
  group_by(loccode1922, svperiod, svdate) %>%
  summarise(points_surveyed = n(),
            playbacks_sum = sum(as.numeric(numberofplaybacks)),
            det_count = sum(as.numeric(det_count)),
            survey_date = 
              first(svdate),
            survey_duration_approx = 
              as.numeric(max(as.POSIXct(svpointtime, format = "%m/%d/%Y, %I:%M %p")) -
                         min(as.POSIXct(svpointtime, format = "%m/%d/%Y, %I:%M %p")),
                         units = "secs"),
            mean_easting = mean(as.numeric(easting)),
            mean_northing = mean(as.numeric(northing)),
            cicada_class = Mode(cicada_index))

table(pt22_sum_site$loccode1922, pt22_sum_site$svperiod)
table(pt22_sum_site$svperiod, pt22_sum_site$det_count)


pt22_sitebysvperiod = pt22_sum_site %>% 
  select(svperiod, det_count, loccode1922) %>%
  pivot_wider(names_from = svperiod, values_from = det_count) %>%
  tibble::column_to_rownames("loccode1922")

pt22_effort_time = pt22_sum_site %>%
  select(svperiod, survey_duration_approx, loccode1922) %>%
  # mutate(effort_time = Scale(survey_duration_approx)) %>%
  pivot_wider(names_from = svperiod, values_from = survey_duration_approx) 

pt22_effort_time_scaled = pt22_effort_time %>%
  select(-1) %>%
  tibble::column_to_rownames("loccode1922") %>%
  scale()

```



## Unmarked data reformat

I started with a simple occupancy model for 2019-2022 data. So I just show how I combined data for those years.

### Sites by Years comparison

```{r sitenames}
(sites1922 = unique(c(row.names(pt19_sitebysvperiod),
      row.names(pt20_sitebysvperiod),
      row.names(pt21_sitebysvperiod),
      row.names(pt22_sitebysvperiod))))

sites1922.df = rbind(data.frame(Year = 2019, 
                                Site = row.names(pt19_sitebysvperiod)),
                     data.frame(Year = 2020, 
                                Site = row.names(pt20_sitebysvperiod)),
                     data.frame(Year = 2021, 
                                Site = row.names(pt21_sitebysvperiod)),
                     data.frame(Year = 2022, 
                                Site = row.names(pt22_sitebysvperiod)))

sites1922.wide.df = sites1922.df %>%
  group_by(Site, Year) %>%
  summarise(n = n()) %>%
  pivot_wider(names_from = Year, values_from = n)

(sites1922.tab = table(sites1922.df$Site, sites1922.df$Year))
```

### Combine years of detection history by site

```{r combineyears}
pt19_sbsp = pt19_sitebysvperiod %>%
  tibble::rownames_to_column("loccode1922")
pt20_sbsp = pt20_sitebysvperiod %>%
  tibble::rownames_to_column("loccode1922")
pt21_sbsp = pt21_sitebysvperiod %>%
  tibble::rownames_to_column("loccode1922")
pt22_sbsp = pt22_sitebysvperiod %>%
  tibble::rownames_to_column("loccode1922")
  
count1922_sitebysvperiod_list = 
  list(pt19_sbsp,
       pt20_sbsp,
       pt21_sbsp,
       pt22_sbsp)

(count1922_sitebysvperiod_lj =  
count1922_sitebysvperiod_list %>%
  Reduce(function(df1, df2)
    left_join(df1, df2, by = "loccode1922"), .))

(svperiodnames1922 =
  names(count1922_sitebysvperiod_lj)[-1] %>%
  as_data_frame() %>%
  separate(value, c("Period", NA, NA)) %>%
  mutate(Period2 = paste0(Period, 
                          c(rep(".2019", 4),
                            rep(".2020", 4),
                            rep(".2021", 4),
                            rep(".2022", 4))
  )))

names(count1922_sitebysvperiod_lj)[-1] = 
    as.character(svperiodnames1922$Period2)


write.csv(count1922_sitebysvperiod_lj,
          "count1922.csv",
          row.names = F)

# Split off sites
names_count1922 = 
  count1922_sitebysvperiod_lj %>%
  select("loccode1922")
cs =
  as.data.frame(count1922_sitebysvperiod_lj) %>%
  select(-"loccode1922")
# convert to presence absence
cs[cs > 0] = 1
# Add sites back on
presabs1922_sitesbyperiod =
  data.frame(names_count1922, cs)
rm(cs)

```

### Detection covariates

Right now, this is just effort calculated as time between first and last point data entry.

We would eventually like to have transect length estimate also.

```{r detection}
class(names_count1922)
class(pt19_effort_time_scaled)

pt19effscaled.df = pt19_effort_time_scaled %>%
  as.data.frame(loccode1922 = rownames(.), .) %>%
  tibble::rownames_to_column()
names(pt19effscaled.df)[1] = "loccode1922"
pt20effscaled.df = pt20_effort_time_scaled %>%
  as.data.frame(loccode1922 = rownames(.), .) %>%
  tibble::rownames_to_column()
names(pt20effscaled.df)[1] = "loccode1922"
pt21effscaled.df = pt21_effort_time_scaled %>%
  as.data.frame(loccode1922 = rownames(.), .) %>%
  tibble::rownames_to_column()
names(pt21effscaled.df)[1] = "loccode1922"
pt22effscaled.df = pt22_effort_time_scaled %>%
  as.data.frame(loccode1922 = rownames(.), .) %>%
  tibble::rownames_to_column()
names(pt22effscaled.df)[1] = "loccode1922"

effort1922_sitebysvperiod_list = 
  list(pt19effscaled.df,
       pt20effscaled.df,
       pt21effscaled.df,
       pt22effscaled.df)

(effort1922_sitebysvperiod_lj =  
effort1922_sitebysvperiod_list %>%
  Reduce(function(df1, df2)
    left_join(df1, df2, by = "loccode1922"), .))

(effperiodnames1922 =
  names(effort1922_sitebysvperiod_lj)[-1] %>%
  as_data_frame() %>%
  separate(value, c("Period", NA, NA)) %>%
  mutate(Period2 = paste0(Period, 
                          c(rep(".2019", 4),
                            rep(".2020", 4),
                            rep(".2021", 4),
                            rep(".2022", 4))
  )))

names(effort1922_sitebysvperiod_lj)[-1] = 
    as.character(effperiodnames1922$Period2)
```

#### Plot effort

Scaled time between first and last playback is reasonably normally distributed with only a view outliers (e.g., 3.2020)

```{r effortplots}
cols = names(effort1922_sitebysvperiod_lj[-1])
effort1922_long = effort1922_sitebysvperiod_lj %>%
  pivot_longer(cols = all_of(cols),
               names_to = "period", 
               values_to = "effort")

effort1922_long %>%
  ggplot( aes(x = effort, color = period, fill = period)) +
    geom_histogram(alpha=0.6, binwidth = 1) +
  facet_wrap(~period)

effort1922_long %>%
  filter(period == "3.2020") %>%
  ggplot( aes(x = effort)) +
    geom_histogram(alpha=0.6, binwidth = 0.2)

```

## Dynamic Occupancy model with effort, year as primary time element

Since I had to summarize detections by site, survey periods are our replicates within year.  Points cannot be used for this because they are a mess to work with.  Eventually, it might be nice to figure out how to use points as replicates and test for survey period effects.  For now I use Chi-Squared tests to show that there is essentially an effect of period on detections with the middle two periods always being best.  There may be a significant Year by survey period interaction.  If you have to choose one period, period 2 is almost always best.

### Simple model with time measure of effort

Model fit is not good and c-hat is almost 2, but we don't have site or yearly site covariates.

```{r occsimple}
library(unmarked)
# remove loccode1922
umf_eff_only = unmarkedMultFrame(y = presabs1922_sitesbyperiod[,-1],
                 numPrimary = 4,
                 obsCovs = list(effort = 
                                  effort1922_sitebysvperiod_lj[,-1])
)

summary(umf_eff_only)


dynamic_occ_m1 = colext( psiformula = ~ 1,
                     gammaformula = ~ 1,
                     epsilonformula = ~ 1,
                     pformula = ~effort,
                     data = umf_eff_only,
                     # optim method, leave as "BFGS"
                     method = "BFGS",
                     control = list(trace=1, maxit=1e4))

# Mackenzie-Bailey GOF test
# Simulate capture history data (if model correct). Compare obs X2 to sim X2
# Must simulate 1000-5000 times for good distribution estimates
# Likely to take a Very Long Time on large data sets
# with nsim = 10, takes my Mackbook ~ 60 seconds
mb.boot <- AICcmodavg::mb.gof.test(dynamic_occ_m1,
                                   nsim = 1000) # Must be much higher than five to be useful

print(mb.boot, digit.vals = 4, digits.chisq = 4)

summary(dynamic_occ_m1)

# The predicted occupancy probability for each site during each primary survey (year). This is stored in an array within the model object (dynamic_occ_m1@smoothed). The array is 2 (a vector of probabilities for 1. unoccupied and 2. occupied) x 5 (number of primary surveys) x 100 (number of sites).

# Occupancy probability for each of 4 years by site

occu_by_site = as.data.frame.matrix(dynamic_occ_m1@smoothed[2,,])
names(occu_by_site) = names_count1922$loccode1922
View(occu_by_site)

# Mean smoothed occupancy probabilities can be accessed using:
# dynamic_occ_m1@smoothed.mean, or
# smoothed(dynamic_occ_m1)

# Calculate SE for derived occupancy predictions using bootstrap, larger B requires longer time
m1 <- nonparboot(dynamic_occ_m1, 
                 B = 1000)

# Predicted occupancy in each year (with SE)
# the "[2,]" calls the occupied estimates,
# "[1,]" for unoccupied estimates
predicted_occupancy <- data.frame(year = c(2019:2022),
                             smoothed_occ = smoothed(dynamic_occ_m1)[2,],
                             SE = m1@smoothed.mean.bsse[2,])
predocc_plot = predicted_occupancy %>%
  ggplot(aes(x = year, y = smoothed_occ)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymin = smoothed_occ - SE, 
                    ymax = smoothed_occ + SE),
                width = 0.2)
print(predocc_plot)
predocc_plot + labs(title="Occupancy Probability 2019-2022", 
                    x="Year", 
                    y = "Occupancy Probability")+
   theme_classic() +
   scale_color_manual(values='#E69F00')
```

### Testing for period effects

These Chi-Squared tests test whether period and the number of sites with one or more detections are independant.  They are significantly associated in years 2019-2020 but not in 2011-2022.

```{r period}
pt19_sum_site$det_count_3plus = 
  as.factor(ifelse(pt19_sum_site$det_count > 2,
                   "3+", pt19_sum_site$det_count))
table(pt19_sum_site$svperiod, pt19_sum_site$det_count)
(pt19_sum_site.tab = as.data.frame.matrix(table(pt19_sum_site$svperiod, pt19_sum_site$det_count_3plus)))
(pt19_sum_site.tab.chi = chisq.test(pt19_sum_site$svperiod, pt19_sum_site$det_count_3plus))

pt20_sum_site$det_count_3plus = 
  as.factor(ifelse(pt20_sum_site$det_count > 2,
                   "3+", pt20_sum_site$det_count))
table(pt20_sum_site$svperiod, pt20_sum_site$det_count)
(pt20_sum_site.tab = as.data.frame.matrix(table(pt20_sum_site$svperiod, pt20_sum_site$det_count_3plus)))
(pt20_sum_site.tab.chi = chisq.test(pt20_sum_site$svperiod, pt20_sum_site$det_count_3plus))

pt21_sum_site$det_count_3plus = 
  as.factor(ifelse(pt21_sum_site$det_count > 2,
                   "3+", pt21_sum_site$det_count))
table(pt21_sum_site$svperiod, pt21_sum_site$det_count)
(pt21_sum_site.tab = as.data.frame.matrix(table(pt21_sum_site$svperiod, pt21_sum_site$det_count_3plus)))
(pt21_sum_site.tab.chi = chisq.test(pt21_sum_site$svperiod, pt21_sum_site$det_count_3plus))

pt22_sum_site$det_count_2plus = 
  as.factor(ifelse(pt22_sum_site$det_count > 1,
                   "2+", pt22_sum_site$det_count))
table(pt22_sum_site$svperiod, pt22_sum_site$det_count)
(pt22_sum_site.tab = as.data.frame.matrix(table(pt22_sum_site$svperiod, pt22_sum_site$det_count_2plus)))
(pt22_sum_site.tab.chi = chisq.test(pt22_sum_site$svperiod, pt22_sum_site$det_count_2plus))

```

